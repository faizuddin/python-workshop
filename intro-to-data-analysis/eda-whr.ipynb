{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7951b4d0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.008914,
     "end_time": "2024-10-04T03:54:25.093956",
     "exception": false,
     "start_time": "2024-10-04T03:54:25.085042",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "\n",
    "## Why EDA?\n",
    "\n",
    "Because in order to start working with our data, we need to know what kind of data we are dealing with. And this detective work got itself the dry name of exploratory data analysis.\n",
    "\n",
    "These are only some of the questions that we ask ourselves. Depending on the answer, we have to proceed with different processing steps before we can use any algorithms on our data:\n",
    "\n",
    "1. Do we have 1000 or 1 million entries in our data?\n",
    "2. Are we dealing with text or numbers?\n",
    "3. Do we have dates? What format to these dates have?\n",
    "4. Do we have outliers? (Data points that are extremely different than all the other ones)\n",
    "5. Do we have missing data? That is, is any of the cells in our dataset empty?\n",
    "\n",
    "If we just open our data, the ``.csv`` file, in a spreadsheet application say Microsoft Excel and look at it with the naked eye, we won't be able to tell much.\n",
    "\n",
    "In order load our data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7329f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T03:54:25.113284Z",
     "iopub.status.busy": "2024-10-04T03:54:25.112287Z",
     "iopub.status.idle": "2024-10-04T03:54:40.856464Z",
     "shell.execute_reply": "2024-10-04T03:54:40.855211Z"
    },
    "papermill": {
     "duration": 15.756934,
     "end_time": "2024-10-04T03:54:40.859344",
     "exception": false,
     "start_time": "2024-10-04T03:54:25.102410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a80a3f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T03:54:40.879407Z",
     "iopub.status.busy": "2024-10-04T03:54:40.878917Z",
     "iopub.status.idle": "2024-10-04T03:54:41.904105Z",
     "shell.execute_reply": "2024-10-04T03:54:41.903182Z"
    },
    "papermill": {
     "duration": 1.038426,
     "end_time": "2024-10-04T03:54:41.906687",
     "exception": false,
     "start_time": "2024-10-04T03:54:40.868261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"datasets/whr-2021.xls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81525679",
   "metadata": {
    "papermill": {
     "duration": 0.008569,
     "end_time": "2024-10-04T03:54:41.924575",
     "exception": false,
     "start_time": "2024-10-04T03:54:41.916006",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Pandas, Numpy, Matplotlib, Seaborn\n",
    "\n",
    "### Pandas\n",
    "In the code right above, we just imported `pandas` library and used `read_csv` to read our csv data in a *Pandas DataFrame*.\n",
    "\n",
    "Pandas is a software library created for data manipulation and analysis. Using `pandas` we can read various file formats easily into data structures specifically created for data manipulation procedures.\n",
    "\n",
    "The most commonly used data structures in pandas are *Series* and *DataFrame*. Series stores one-dimensional data (like a table with only one column) and DataFrame stores 2-dimensional data (tables with multiple columns).\n",
    "\n",
    "We are not going into much detail on pandas but the best place to learn pandas is the [official documentation](https://pandas.pydata.org/), but if during or after reading this you feel like you need a more thorough work session with pandas, please have a look at this [10 minutes](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html) to pandas tutorial (or way more than 10 minutes).\n",
    "\n",
    "### Numpy\n",
    "[Numpy](https://numpy.org/) is a library mainly used for the Mathematical functions it implements. This way we don't have to write the functions ourselves all the time.\n",
    "\n",
    "### Matplotlib\n",
    "Matplotlib brings us data visualisations.\n",
    "\n",
    "### Seaborn\n",
    "[Seaborn](https://seaborn.pydata.org) takes visualisations to the next level: more powerful and more beautiful, and perhaps more abstract than Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90070917",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T03:54:41.945379Z",
     "iopub.status.busy": "2024-10-04T03:54:41.943948Z",
     "iopub.status.idle": "2024-10-04T03:54:41.979486Z",
     "shell.execute_reply": "2024-10-04T03:54:41.978316Z"
    },
    "papermill": {
     "duration": 0.048385,
     "end_time": "2024-10-04T03:54:41.981956",
     "exception": false,
     "start_time": "2024-10-04T03:54:41.933571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's set the precision to 4 decimal places\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "\n",
    "# The first 3 rows of our pandas DataFrame object. If we run df.head(), it will display the first 5 rows by default.To display the first 3 rows...\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c15359",
   "metadata": {
    "papermill": {
     "duration": 0.009001,
     "end_time": "2024-10-04T03:54:42.001288",
     "exception": false,
     "start_time": "2024-10-04T03:54:41.992287",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Pandas makes it very easy to handle tabular data.\n",
    "\n",
    "Tabular data means that our data fits or belongs in a table. Other types of data can be visual (that is, images, for which it doesn't really make sense to be stored as csv files).\n",
    "\n",
    "The standard way to store tabular data is that:\n",
    "\n",
    "* Each row represents a different observation. \n",
    "* Observation is a fancy Statistics term, but it just means a new data point, a new measurement did by this group of [researchers](https://worldhappiness.report/about/). \n",
    "\n",
    "#### Row and column\n",
    "If our data is about happiness in various countries, each row contains data for a new country. Each column is a different feature (or attribute) of our observations. For the World Happiness Report dataset, examples of features can be the `Country` name, the `Regional indicator` or the `Social Support score`.\n",
    "\n",
    "Now, let's use the `numpy` library to see the maximum value of the feature Ladder score across all observations in our dataset (all countries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175f01c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T03:54:42.022421Z",
     "iopub.status.busy": "2024-10-04T03:54:42.021441Z",
     "iopub.status.idle": "2024-10-04T03:54:42.031603Z",
     "shell.execute_reply": "2024-10-04T03:54:42.030501Z"
    },
    "papermill": {
     "duration": 0.023198,
     "end_time": "2024-10-04T03:54:42.033854",
     "exception": false,
     "start_time": "2024-10-04T03:54:42.010656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's import the numpy library\n",
    "import numpy as np\n",
    "\n",
    "# Use a numpy function to see what's the maximum value for our Ladder score feature\n",
    "np.max(df[\"Ladder score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92858e7e",
   "metadata": {
    "papermill": {
     "duration": 0.009353,
     "end_time": "2024-10-04T03:54:42.052841",
     "exception": false,
     "start_time": "2024-10-04T03:54:42.043488",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "And since we're here, let's see how convenient it is to use pandas *DataFrame* structure. We found the maximum values for \"Ladder score\" feature, but what is the row number of the entry with the `max()` *Ladder score*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc91a95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T03:54:42.074114Z",
     "iopub.status.busy": "2024-10-04T03:54:42.073055Z",
     "iopub.status.idle": "2024-10-04T03:54:42.080861Z",
     "shell.execute_reply": "2024-10-04T03:54:42.079829Z"
    },
    "papermill": {
     "duration": 0.020895,
     "end_time": "2024-10-04T03:54:42.083167",
     "exception": false,
     "start_time": "2024-10-04T03:54:42.062272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Ladder score'].argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8dfbfe",
   "metadata": {
    "papermill": {
     "duration": 0.009262,
     "end_time": "2024-10-04T03:54:42.102145",
     "exception": false,
     "start_time": "2024-10-04T03:54:42.092883",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "It only took one line of code to find the row number. Let's see this observation's features, to convince ourselves we got the right entry. Note that when displaying one single entry from the DataFrame, the feature values won't appear 0 a row anymore, but will be displayed as a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1acaf5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T03:54:42.123426Z",
     "iopub.status.busy": "2024-10-04T03:54:42.122972Z",
     "iopub.status.idle": "2024-10-04T03:54:42.132356Z",
     "shell.execute_reply": "2024-10-04T03:54:42.131270Z"
    },
    "papermill": {
     "duration": 0.022902,
     "end_time": "2024-10-04T03:54:42.134696",
     "exception": false,
     "start_time": "2024-10-04T03:54:42.111794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.iloc[df['Ladder score'].argmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9832ea",
   "metadata": {
    "papermill": {
     "duration": 0.009465,
     "end_time": "2024-10-04T03:54:42.153951",
     "exception": false,
     "start_time": "2024-10-04T03:54:42.144486",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data types\n",
    "\n",
    "We have some idea about or features types just by looking at the .csv file. But a better and **systemic** method is the one below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f2200a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T03:54:42.175711Z",
     "iopub.status.busy": "2024-10-04T03:54:42.174900Z",
     "iopub.status.idle": "2024-10-04T03:54:42.199114Z",
     "shell.execute_reply": "2024-10-04T03:54:42.198093Z"
    },
    "papermill": {
     "duration": 0.038684,
     "end_time": "2024-10-04T03:54:42.202405",
     "exception": false,
     "start_time": "2024-10-04T03:54:42.163721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DataFrame has this very handy method.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be5bbe1",
   "metadata": {
    "papermill": {
     "duration": 0.009584,
     "end_time": "2024-10-04T03:54:42.221955",
     "exception": false,
     "start_time": "2024-10-04T03:54:42.212371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "What do we see in the output above?\n",
    "\n",
    "* Our data is a DataFrame, with 149 entries (from 0 to 148)\n",
    "* We have 20 columns (from 0 to 19)\n",
    "* All of the columns have 149 non-null values (we don't have **missing** data in any of these columns)\n",
    "* Column types are: object (2 of them) and float64 (18 of them). *float64 means they can store fractional numbers and each number takes 64 bits*\n",
    "\n",
    "* The 'object' type we see above most likely refers to a string. We'll use [DataFrame indexing and selection](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html) to look at one particular value to verify our assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261ee2e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T03:54:42.243815Z",
     "iopub.status.busy": "2024-10-04T03:54:42.243411Z",
     "iopub.status.idle": "2024-10-04T03:54:42.249432Z",
     "shell.execute_reply": "2024-10-04T03:54:42.248311Z"
    },
    "papermill": {
     "duration": 0.019966,
     "end_time": "2024-10-04T03:54:42.251939",
     "exception": false,
     "start_time": "2024-10-04T03:54:42.231973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(df['Country name'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f16bc6",
   "metadata": {
    "papermill": {
     "duration": 0.010304,
     "end_time": "2024-10-04T03:54:42.272904",
     "exception": false,
     "start_time": "2024-10-04T03:54:42.262600",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Ok, so, in this case, 'object' means String."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6bee41",
   "metadata": {
    "papermill": {
     "duration": 0.009772,
     "end_time": "2024-10-04T03:54:42.292724",
     "exception": false,
     "start_time": "2024-10-04T03:54:42.282952",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Exploring categorical variable\n",
    "\n",
    "We have 2 feature which contain text:\n",
    "\n",
    "1. Country\n",
    "2. Region\n",
    "\n",
    "### Country\n",
    "Our intuition is that each country is unique in our dataset (one country per row). This is what we would expect from a study of happiness levels in different countries across the world. We can verify this assumption, to make sure we don't have errors in our data. For example, the social scientist running this study could have accidentally entered the same observation twice because she was working late to finish her data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4cfd60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T03:54:42.314593Z",
     "iopub.status.busy": "2024-10-04T03:54:42.314185Z",
     "iopub.status.idle": "2024-10-04T03:54:42.324514Z",
     "shell.execute_reply": "2024-10-04T03:54:42.323409Z"
    },
    "papermill": {
     "duration": 0.024286,
     "end_time": "2024-10-04T03:54:42.327027",
     "exception": false,
     "start_time": "2024-10-04T03:54:42.302741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# How many entries we have for each country shown in descending order (highest value first)\n",
    "df[\"Country name\"].value_counts().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ff8975",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T03:54:42.349310Z",
     "iopub.status.busy": "2024-10-04T03:54:42.348827Z",
     "iopub.status.idle": "2024-10-04T03:54:42.355494Z",
     "shell.execute_reply": "2024-10-04T03:54:42.354552Z"
    },
    "papermill": {
     "duration": 0.021228,
     "end_time": "2024-10-04T03:54:42.358398",
     "exception": false,
     "start_time": "2024-10-04T03:54:42.337170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment the line below to see what data type we used. This is a nice way to explore the functioning of pandas.\n",
    "print(\"\\nThe code above returns a date of type: \", type(df['Country name'].value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b3f837",
   "metadata": {
    "papermill": {
     "duration": 0.01007,
     "end_time": "2024-10-04T03:54:42.378875",
     "exception": false,
     "start_time": "2024-10-04T03:54:42.368805",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Region\n",
    "Let's have a look at the regions now. It would be interesting to see what different regions we have. This would open the door for questions like: *Are people happier in Western Europen than in Eastern Europe?*. We don't know yet what question we can ask and exploring our data informs our next steps.\n",
    "\n",
    "By the way, since we are dealing with long column names, it's worth mentioning that we don't have to type the whole column name. I just input the first 3 letters and press Tab for autocomplete.\n",
    "\n",
    "We see in the output below that:\n",
    "\n",
    "* Europe is split into 2: **Western Europe** and **Central and Eastern Europe**\n",
    "* The Americas are divided into 2: **Latin America and Caribbean** and **North America and ANZ** (which is North America, Australia and New Zealand)\n",
    "* Africa is split into 2: **Sub-Saharan Africa** and **Middle East and North Africa**\n",
    "* Asia is divided into 3: **Southeast Asia**, **South Asia** and **East Asia**\n",
    "* There is a group of post-Soviet republics in Eurasia making up the **Commonwealth of Independent States**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df17e9b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T03:54:42.402241Z",
     "iopub.status.busy": "2024-10-04T03:54:42.401792Z",
     "iopub.status.idle": "2024-10-04T03:54:42.410641Z",
     "shell.execute_reply": "2024-10-04T03:54:42.409563Z"
    },
    "papermill": {
     "duration": 0.023764,
     "end_time": "2024-10-04T03:54:42.413585",
     "exception": false,
     "start_time": "2024-10-04T03:54:42.389821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here's each individual region and its corresponding frequency (the statistical term for the number of times this region appears in our dataset)\n",
    "\n",
    "df['Regional indicator'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d930a092",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T03:54:42.436195Z",
     "iopub.status.busy": "2024-10-04T03:54:42.435736Z",
     "iopub.status.idle": "2024-10-04T03:54:42.442405Z",
     "shell.execute_reply": "2024-10-04T03:54:42.441187Z"
    },
    "papermill": {
     "duration": 0.020729,
     "end_time": "2024-10-04T03:54:42.444730",
     "exception": false,
     "start_time": "2024-10-04T03:54:42.424001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Method to find this out total number of regions in our dataset\n",
    "print(f\"The number of regions in our dataset is: {df['Regional indicator'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc4ed33",
   "metadata": {
    "papermill": {
     "duration": 0.010233,
     "end_time": "2024-10-04T03:54:42.465690",
     "exception": false,
     "start_time": "2024-10-04T03:54:42.455457",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The above line of code uses Python's fancy formatting called *Literal String Interpolation* (but the popular name is f-string). You can read more [here](https://www.programiz.com/python-programming/string-interpolation).\n",
    "\n",
    "### Visualisation for categorical features\n",
    "Since the frequencies (the number of times they appear in our dataset) of our regions is greater than one, it invites us to look at them in a more intuitive way rather than the text displayed above.\n",
    "\n",
    "It is generally much better for the audience to present any data in visual form, whenever possible. For countries, nothing else made sense since each country appeared once in our data. But for regions, we can use a bar chart.\n",
    "\n",
    "The bar chart below shows the same information as the table we've seen earlier.\n",
    "But in visual form it's so much easier to gain insights like **Sub-Saharan Africa** is present in our dataset approximately twice as much as the next region in line, **Western Europe**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650f6b0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T03:54:42.488474Z",
     "iopub.status.busy": "2024-10-04T03:54:42.488030Z",
     "iopub.status.idle": "2024-10-04T03:54:42.943506Z",
     "shell.execute_reply": "2024-10-04T03:54:42.942523Z"
    },
    "papermill": {
     "duration": 0.470328,
     "end_time": "2024-10-04T03:54:42.946535",
     "exception": false,
     "start_time": "2024-10-04T03:54:42.476207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Regional indicator'].value_counts().plot(kind='bar', title='Absolute frequency distribution of Regional indicator')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b661274b",
   "metadata": {
    "papermill": {
     "duration": 0.011119,
     "end_time": "2024-10-04T03:54:42.969829",
     "exception": false,
     "start_time": "2024-10-04T03:54:42.958710",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Another obsvervation for the plot above is that those numbers are absolute frequencies. That is, the bar chart shows the number of times each region is present in our dataset. Sometimes it's enough to know that we have 39 countries from **Sub-Saharan Africa**. But there are times when we're wondering how much this represents in terms of percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b99f9b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T03:54:42.994633Z",
     "iopub.status.busy": "2024-10-04T03:54:42.994203Z",
     "iopub.status.idle": "2024-10-04T03:54:43.379987Z",
     "shell.execute_reply": "2024-10-04T03:54:43.378886Z"
    },
    "papermill": {
     "duration": 0.401134,
     "end_time": "2024-10-04T03:54:43.382418",
     "exception": false,
     "start_time": "2024-10-04T03:54:42.981284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "(df['Regional indicator'].value_counts()/df.shape[0]).plot(kind='bar', title='Relative frequency of Regional indicators')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c85aa28",
   "metadata": {
    "papermill": {
     "duration": 0.012491,
     "end_time": "2024-10-04T03:54:43.407928",
     "exception": false,
     "start_time": "2024-10-04T03:54:43.395437",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now we know that **Sub-Saharan Africa** represents 25% of our data. \n",
    "\n",
    "For this dataset this is not unusual. But imagine if you're trying to see how happy people are in a single country, you broadcast a digital survey that people can take and during data analysis you realise that 25% of the people who filled in the survey are from the same city in this country.\n",
    "\n",
    "So how do we go about this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b67514f",
   "metadata": {},
   "source": [
    "## Exploring Numerical Features\n",
    "\n",
    "Pandas has a nice built-in method that performs descriptive statistics on a DataFrame.\n",
    "\n",
    "It shows us:\n",
    "\n",
    "1. number of values for each feature (we could see if we have missing values for any feature)\n",
    "2. mean value\n",
    "3. standard error\n",
    "4. min and max value\n",
    "5. median of our data (50%)\n",
    "6. lower and upper quartile (25% and 75%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebee1ea",
   "metadata": {
    "papermill": {
     "duration": 0.012317,
     "end_time": "2024-10-04T03:54:43.433218",
     "exception": false,
     "start_time": "2024-10-04T03:54:43.420901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df. describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faccb9f",
   "metadata": {},
   "source": [
    "### Insights from statistics above\n",
    "\n",
    "1. Ladder score actually goes from $2.5$ to $7.8$. There's no $0$ or $10$.\n",
    "2. Healthy life expectancy has a minimum of $45$ and a maximum of $76$. This is a large range. There are countries in our dataset where life expenctancy is $45$ years!\n",
    "3. Generosity can be negative. It's the only feature that has negative values.\n",
    "4. Other features are more difficult to interpret from the descriptive stats above.\n",
    "\n",
    "Numerical data is best viewed as histograms. We will use both [matplotlib](https://matplotlib.org) and [seaborn](https://seaborn.pydata.org) for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7255f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# column name in our dataset\n",
    "columns = ['Logged GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption']\n",
    "\n",
    "# determine number of rows\n",
    "scols = int(len(columns)/2)\n",
    "srows = 2\n",
    "\n",
    "# grid of plots\n",
    "fig, axes = plt.subplots(scols, srows, figsize=(10,6))\n",
    "\n",
    "for i, col in enumerate(columns):\n",
    "    ax_col = int(i%scols)\n",
    "    ax_row = int(i/scols)\n",
    "    \n",
    "    sns.histplot(data=df[col], kde=True, stat=\"density\", ax=axes[ax_col, ax_row])\n",
    "    axes[ax_col, ax_row].set_title('Frequency distribution '+ col, fontsize=12)\n",
    "    axes[ax_col, ax_row].set_xlabel(col, fontsize=8)\n",
    "    axes[ax_col, ax_row].set_ylabel('Count', fontsize=8)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2424c87c",
   "metadata": {},
   "source": [
    "### Insights from the visual exploration of our numerical data (histogram)\n",
    "\n",
    "1. Distributions of GDP, social support, healthy life expectancy, freedom and corruption are all left skewed (or negative skew). \n",
    "2. In other words, most of the values do not happen to be in the middle of the *min-max* range, but are pushed towards the upper end of the range - happen for all but **Perception of corruption**. This is good news.\n",
    "3. Generosity, though, is right skewed. The majority of the countries are in the bottom half of the generosity scale (unfortunately)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32f292c",
   "metadata": {},
   "source": [
    "## Bivariate analysis\n",
    "\n",
    "All the explorations above belong to [univariate](https://en.wikipedia.org/wiki/Univariate_(statistics)) analysis (that is, we looked at each variable individually). We can also perform [bivariate analysis](https://en.wikipedia.org/wiki/Bivariate_analysis), which instead of looking individually, we can look at pairs of two variables to explore a possible relation between them.\n",
    "\n",
    "The ideal way to perform this by is using scatterplots to search for clouds of dots that arrange themselves into straight diagonal lines. This is a visual representation of two variables that correlate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca01924",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Logged GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption']\n",
    "\n",
    "# do scatterplots\n",
    "sns.pairplot(df[columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685fc05a",
   "metadata": {},
   "source": [
    "### How to read the plots above?\n",
    "\n",
    "Let's look at the second plot on the first row. On the far left of the image we see *Logged GDP per capita*. All plots on the first row have on the y axis (the vertical axis) the *Logged GDP per capita* as the label of the Y axis. Now look at the bottom of the plots, all the way down, under the second column we have *Social support* as the name of the X axis. All plots on the second columns have the *Social support* on the x axis (the horizontal axis).\n",
    "\n",
    "Now, let's look at the contents of the second plot, first row. As the *Social support* increases, so does *Logged GDP per capita*. What does this mean? This tells that the two feature seems to be correlated (correlation, not causation). Most likely (based on assumptions) as the **country gets riches it can afford to offer more social support to its people**.\n",
    "\n",
    "Now look at the fourth subplot on the same row. The datapoints are all over place and there seems to be no correlation between *GDP per capita* and *Freedom to make life choices*.\n",
    "\n",
    "Correlation is not assessed only by looking at a scatterplots - but this is a good start. We are exploring after all.\n",
    "\n",
    "Finally look on the diagonal, from upper left to lower right. These are the histograms we've plotted earlier.\n",
    "\n",
    "#### Hue\n",
    "\n",
    "We also can assign different colours to datapoints that belong to different global regions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2c784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Regional indicator','Logged GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity','Perceptions of corruption']\n",
    "\n",
    "# do scatterplots with hue\n",
    "sns.pairplot(df[columns], hue=\"Regional indicator\", palette=\"Paired\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04d10ff",
   "metadata": {},
   "source": [
    "The *colourisation* could help us gain to insight like: Sub-Saharan African countries (the purple dots, according to the legend on the right) have the **lowest GDP and the lowest Healthy life expectancy, but they are not less generous than more fortunate countries**.\n",
    "\n",
    "### Correlation\n",
    "\n",
    "[Correlation](https://en.wikipedia.org/wiki/Correlation) is not assessed only by looking at a scatterplots, but using another useful EDA toolset: the correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208e8f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_columns = ['Ladder score','Logged GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity','Perceptions of corruption']\n",
    "\n",
    "corr = df[meaningful_columns].corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06336083",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = corr.map(lambda v: v if v else '')\n",
    "sns.heatmap(corr, annot=labels, fmt=\".2g\", cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c9f39e",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "\n",
    "To spot outliers in our dataset we can use [Box and Whiskers](https://en.wikipedia.org/wiki/Box_plot) plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9559e1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "small = ['Social support', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption']\n",
    "medium = ['Ladder score', 'Logged GDP per capita']\n",
    "large = ['Healthy life expectancy']\n",
    "\n",
    "f, axs = plt.subplots(1,3,figsize=(15,5))\n",
    "\n",
    "# equivalent but more general\n",
    "ax1=plt.subplot(1, 3, 1)\n",
    "df.boxplot(column=small, ax = ax1)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "ax2=plt.subplot(1, 3, 2)\n",
    "df.boxplot(column=medium, ax = ax2)\n",
    "\n",
    "ax3=plt.subplot(1, 3, 3)\n",
    "df.boxplot(column=large, ax = ax3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a6f3b7",
   "metadata": {},
   "source": [
    "The classical interpretation in Statistics is that whatever falls outside the *whiskers* represents an outlier. In practice, deciding what to do with outliers depends on many factors (it also could be a mistake in data collection, for example).\n",
    "\n",
    "### Perceptions of corruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540ad249",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1,2,figsize=(12,4))\n",
    "\n",
    "# equivalent but more general\n",
    "ax1=plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['Perceptions of corruption'], kde=True, ax=ax1)\n",
    "\n",
    "ax2=plt.subplot(1, 2, 2)\n",
    "df.boxplot(column=['Perceptions of corruption'], ax = ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f763dafb",
   "metadata": {},
   "source": [
    "Because *Perceptions of corruption* feature is left skewed, countries with lowest perception of corruption are automatically categorised as outliers in the boxplot. But just because they are technically outliers does not necessarily mean we should do something about them. \n",
    "\n",
    "### Is the data correct? \n",
    "\n",
    "Let's see who these outliers are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38615631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select rows with \"Perceptions of corruption\" < 0.4\n",
    "rows = df[df['Perceptions of corruption'] < 0.4]\n",
    "\n",
    "# select columns 'Country name', 'Perceptions of corruption' from the rows\n",
    "rows = rows[['Country name', 'Perceptions of corruption']]\n",
    "\n",
    "# sort the the rows\n",
    "rows.sort_values(by = 'Perceptions of corruption', axis=0, ascending=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5591a2",
   "metadata": {},
   "source": [
    "It's no surprise to find almost all these countries in the bottom of the Perceptions of Corruption.\n",
    "\n",
    "What is the *Perceptions of corruption* for our country?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376e9ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = df[df['Country name'] == \"Malaysia\"]\n",
    "rows[['Country name', 'Perceptions of corruption']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d07dde3",
   "metadata": {},
   "source": [
    "### That's basically EDA in a nutshell."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5802641,
     "sourceId": 9528663,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5802701,
     "sourceId": 9528742,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30775,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 21.97505,
   "end_time": "2024-10-04T03:54:43.966920",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-04T03:54:21.991870",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
